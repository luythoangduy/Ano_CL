# SEMA Integration for AnoCL

## ğŸ¯ Tá»•ng quan

TÃ­ch há»£p **SEMA (Self-Expansion of pre-trained models with Mixture of Adapters)** tá»« CVPR 2025 vÃ o **AnoCL** Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng Continual Learning cho Anomaly Detection.

### Äiá»ƒm khÃ¡c biá»‡t so vá»›i Baseline CL

| Feature | Baseline CL | SEMA-CL |
|---------|------------|---------|
| **Adapters** | âŒ KhÃ´ng cÃ³ | âœ… Self-expanding adapters |
| **Anti-forgetting** | âŒ KhÃ´ng cÃ³ | âœ… Adapter freezing + Router mixing |
| **Distribution shift detection** | âŒ KhÃ´ng cÃ³ | âœ… Representation Descriptor (RD) |
| **Model expansion** | âŒ Fixed | âœ… Dynamic (auto-expand khi cáº§n) |
| **Forgetting** | ğŸ”´ Cao (~0.20) | ğŸŸ¢ Tháº¥p hÆ¡n (dá»± kiáº¿n) |

---

## ğŸ—ï¸ Kiáº¿n trÃºc SEMA

### 1. **Components chÃ­nh**

```
Input Features
    â†“
Transformer Encoder (4 layers)
    â”œâ”€ Self-Attention
    â”œâ”€ **SEMA Adapter** [NEW]
    â”œâ”€ Feedforward
    â””â”€ **SEMA Adapter** [NEW]
    â†“
Memory Module
    â†“
Transformer Decoder (4 layers)
    â”œâ”€ Self-Attention
    â”œâ”€ **SEMA Adapter** [NEW]
    â”œâ”€ Cross-Attention
    â”œâ”€ **SEMA Adapter** [NEW]
    â”œâ”€ Feedforward
    â””â”€ **SEMA Adapter** [NEW]
    â†“
Output
```

### 2. **SEMA Adapter Module**

Má»—i adapter gá»“m 2 pháº§n:

#### **a) Functional Adapter** (Bottleneck)
```python
input [256] â†’ down_proj [64] â†’ ReLU â†’ up_proj [256] â†’ output
```

#### **b) Representation Descriptor (RD)** (AutoEncoder)
```python
input [256] â†’ encoder [64] â†’ decoder [256] â†’ reconstruction
```

**Má»¥c Ä‘Ã­ch RD:**
- Train: Learn to reconstruct normal features
- Test: Detect distribution shift
  - High reconstruction error â†’ Z-score cao â†’ **Trigger expansion**

### 3. **Self-Expansion Mechanism**

```python
# Compute Z-score
z_score = (rd_loss - mean) / std

# Expansion criteria
if z_score.mean() > threshold (default: 3.0):
    âœ¨ Add new adapter
    ğŸ“Š Update router (mix adapter outputs)
```

---

## ğŸ“ Files Ä‘Æ°á»£c táº¡o

### **1. Core Components**

| File | MÃ´ táº£ |
|------|-------|
| [`models/sema_components.py`](Ano_CL/models/sema_components.py) | Adapter, RD, RDLossRecords |
| [`models/sema_modules.py`](Ano_CL/models/sema_modules.py) | SEMAModules (manager cho multiple adapters) |

### **2. Transformer vá»›i SEMA**

| File | MÃ´ táº£ |
|------|-------|
| [`models/reconstructions/dumenet_sema.py`](Ano_CL/models/reconstructions/dumenet_sema.py) | SEMA Encoder/Decoder Layers |
| [`models/reconstructions/uniad_sema.py`](Ano_CL/models/reconstructions/uniad_sema.py) | UniADMemorySEMA (full model) |

### **3. Training**

| File | MÃ´ táº£ |
|------|-------|
| [`models/uniad_sema_learner.py`](Ano_CL/models/uniad_sema_learner.py) | SEMA Learner vá»›i RD loss |
| [`tools/train_sema.py`](Ano_CL/tools/train_sema.py) | Script huáº¥n luyá»‡n SEMA |
| [`tools/config_sema.yaml`](Ano_CL/tools/config_sema.yaml) | Config cho SEMA |

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### **1. CÃ i Ä‘áº·t dependencies**

```bash
cd Ano_CL
pip install -r requirements.txt
```

### **2. Chuáº©n bá»‹ data**

Äáº£m báº£o MVTec-AD dataset Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ (xem [`README_CL.md`](Ano_CL/README_CL.md))

### **3. Chá»‰nh config**

Edit [`tools/config_sema.yaml`](Ano_CL/tools/config_sema.yaml):

```yaml
# SEMA settings
sema:
  use_sema: true

  # Adapter position: 'ffn', 'attn', or 'both'
  sema_position: 'ffn'

  # Adapter mode: 'parallel' (residual) or 'sequential'
  sema_mode: 'parallel'

  # Expansion threshold (Z-score)
  expansion_threshold: 3.0

  # RD loss weight
  rd_loss_weight: 0.1
```

### **4. Cháº¡y training**

```bash
cd tools
python train_sema.py --config config_sema.yaml
```

**Options:**
```bash
# Custom seed
python train_sema.py --config config_sema.yaml --seed 42

# Specific GPU
python train_sema.py --config config_sema.yaml --device 0

# Multi-GPU
python train_sema.py --config config_sema.yaml --device 0 1
```

---

## ğŸ“Š Output

### **1. Logs**
```
logs_sema/sema_train_TIMESTAMP.log
```

**Bao gá»“m:**
- Training loss per epoch
- **RD loss** (representation descriptor)
- **Adapter statistics** (sá»‘ lÆ°á»£ng adapters per layer)
- **Expansion events** (khi nÃ o adapter Ä‘Æ°á»£c thÃªm)
- Evaluation metrics per task

### **2. Checkpoints**
```
checkpoints_sema/model_task_X.pkl
```

### **3. Results**
```
results_sema/sema_results_TIMESTAMP.json
```

**Format:**
```json
{
  "config": {
    "sema_config": {...},
    "num_tasks": 15,
    ...
  },
  "task_performance": {...},
  "cl_metrics": [...],
  "adapter_stats": [
    {
      "total_adapters": 8,
      "adapters_per_layer": [
        {"layer_id": 0, "num_adapters": 1},
        {"layer_id": 1, "num_adapters": 2},  â† Expanded
        ...
      ]
    },
    ...
  ]
}
```

---

## âš™ï¸ Hyperparameters chÃ­nh

### **SEMA Adapters**

| Parameter | Default | Ã nghÄ©a |
|-----------|---------|---------|
| `adapter_bottleneck` | 64 | Dimension cá»§a adapter bottleneck |
| `adapter_dropout` | 0.1 | Dropout rate |
| `adapter_scalar` | 1.0 | Scaling factor cho adapter output |
| `sema_position` | 'ffn' | Vá»‹ trÃ­ thÃªm adapter ('ffn', 'attn', 'both') |
| `sema_mode` | 'parallel' | Mode ('parallel'=residual, 'sequential'=in-place) |

### **Representation Descriptor**

| Parameter | Default | Ã nghÄ©a |
|-----------|---------|---------|
| `rd_dim` | 64 | RD bottleneck dimension |
| `rd_buffer_size` | 500 | Sá»‘ samples Ä‘á»ƒ tÃ­nh mean/std |
| `expansion_threshold` | 3.0 | Z-score threshold Ä‘á»ƒ trigger expansion |
| `rd_loss_weight` | 0.1 | Weight cá»§a RD loss trong total loss |

### **Layer Range**

| Parameter | Default | Ã nghÄ©a |
|-----------|---------|---------|
| `sema_start_layer` | 0 | Layer Ä‘áº§u tiÃªn cÃ³ adapters |
| `sema_end_layer` | 7 | Layer cuá»‘i cÃ¹ng cÃ³ adapters (4 encoder + 4 decoder = 8 layers) |

---

## ğŸ”¬ So sÃ¡nh vá»›i Baseline

### **Training Process**

| Step | Baseline CL | SEMA-CL |
|------|------------|---------|
| Task 0 | Train all params | Train adapters (init: 1 per layer) |
| After Task 0 | Continue | **Freeze adapters**, enable outlier detection |
| Task 1 | Train all params | Detect shift â†’ **Expand if needed**, train new adapters |
| ... | Catastrophic forgetting | **Minimal forgetting** (frozen adapters) |

### **Expected Results**

| Metric | Baseline | SEMA (expected) |
|--------|----------|-----------------|
| Final Avg Performance | ~0.74 | ~0.78-0.82 |
| Final Avg Forgetting | ~0.20 | ~0.10-0.15 |
| Model size | Fixed | **Sub-linear expansion** |

---

## ğŸ“ CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng chi tiáº¿t

### **1. Task 0 - Khá»Ÿi táº¡o**

```python
# Má»—i layer cÃ³ 1 adapter
Layer 0: [Adapter_0]
Layer 1: [Adapter_0]
...
Layer 7: [Adapter_0]

# Train adapters + RD
Loss = MSE_loss + 0.1 * RD_loss
```

### **2. After Task 0**

```python
# Freeze táº¥t cáº£ adapters
for adapter in adapters:
    adapter.freeze_functional()  # Freeze adapter weights
    adapter.freeze_rd()           # Freeze RD weights
    adapter.rd_loss_record.freeze()  # Stop updating statistics

# Enable outlier detection
model.enable_outlier_detection()
```

### **3. Task 1 - Outlier Detection**

```python
# Forward pass
for sample in task_1_data:
    rd_loss = RD.compute_loss(sample)
    z_score = (rd_loss - mean) / std

    if z_score > 3.0:
        âœ¨ Add new adapter to this layer
        Layer_i: [Adapter_0, Adapter_1]  â† NEW
```

### **4. Task 1 - Mixing Adapters**

```python
# Router network
logits = Router(input)  # [batch, num_adapters]
weights = softmax(logits)

# Mix adapter outputs
output = Î£ (weights[i] * Adapter_i(input))
```

### **5. After Task 1**

```python
# Freeze new adapters
# Enable outlier detection for Task 2
...
```

---

## ğŸ’¡ Tips & Tricks

### **1. Äiá»u chá»‰nh Expansion Threshold**

```yaml
# Conservative (expand Ã­t hÆ¡n)
expansion_threshold: 4.0  # Chá»‰ expand khi shift ráº¥t lá»›n

# Aggressive (expand nhiá»u hÆ¡n)
expansion_threshold: 2.0  # Expand dá»… hÆ¡n
```

**Trade-off:**
- High threshold â†’ Ãt adapters â†’ Faster, nhÆ°ng cÃ³ thá»ƒ underfit new tasks
- Low threshold â†’ Nhiá»u adapters â†’ Slower, nhÆ°ng better adaptation

### **2. Äiá»u chá»‰nh RD Loss Weight**

```yaml
# TÄƒng RD loss â†’ RD há»c tá»‘t hÆ¡n â†’ Detection chÃ­nh xÃ¡c hÆ¡n
rd_loss_weight: 0.2

# Giáº£m RD loss â†’ Focus vÃ o main task
rd_loss_weight: 0.05
```

### **3. Chá»n Adapter Position**

```yaml
# FFN only (faster, fewer adapters)
sema_position: 'ffn'

# Attention only (different feature space)
sema_position: 'attn'

# Both (maximum flexibility, more adapters)
sema_position: 'both'
```

---

## ğŸ› Troubleshooting

### **Problem: KhÃ´ng cÃ³ expansion**

**NguyÃªn nhÃ¢n:**
- Threshold quÃ¡ cao
- RD chÆ°a há»c tá»‘t (rd_loss_weight quÃ¡ tháº¥p)

**Giáº£i phÃ¡p:**
```yaml
expansion_threshold: 2.5  # Giáº£m threshold
rd_loss_weight: 0.15      # TÄƒng RD loss weight
```

### **Problem: Expansion quÃ¡ nhiá»u**

**NguyÃªn nhÃ¢n:**
- Threshold quÃ¡ tháº¥p
- RD overfitting

**Giáº£i phÃ¡p:**
```yaml
expansion_threshold: 3.5  # TÄƒng threshold
rd_buffer_size: 1000      # TÄƒng buffer size (stable statistics)
```

### **Problem: Training cháº­m**

**Giáº£i phÃ¡p:**
```yaml
# Giáº£m sá»‘ layers cÃ³ adapters
sema_start_layer: 2
sema_end_layer: 5

# Hoáº·c chá»‰ dÃ¹ng FFN
sema_position: 'ffn'
```

---

## ğŸ“š References

1. **SEMA Paper** (CVPR 2025):
   - Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning
   - https://arxiv.org/abs/2403.18886

2. **UniAD** (Anomaly Detection):
   - Unified Anomaly Detection Framework

3. **AnoCL** (Baseline):
   - Continual Learning for Anomaly Detection on MVTec-AD

---

## ğŸ“§ Contact

For questions or issues, please open an issue on the repository.

---

**Happy Training! ğŸš€**
